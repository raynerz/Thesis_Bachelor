@techreport{Cochrane2005,
abstract = {Figure out the one central and novel contribution of your paper. Write this down in one paragraph. As with all your writing, this must be concrete. Don't write "I analyzed data on executive compensation and found many interesting results." Explain what the central results are. For example, Fama and French 1992 start their abstract with: "Two easily measured variables, size and book-to-market equity, combine to capture the cross-sectional variation in average stock returns associated with market , size, leverage, book-to-market equity, and earnings-price ratios." Distilling your one central contribution will take some thought. It will cause some pain, because you will start to realize how much you're going to have to throw out. Once you do it, though, you're in a much better position to focus the paper on that one contribution, and help readers to get it quickly. Your readers are busy and impatient. No reader will ever read the whole thing from start to nish. Readers skim. You have to make it easy for them to skim. Most readers want to know your basic result. Only a few care how it is digerent from others. Only a few care if it holds up with digerent variable denitions, digerent instrument sets, etc. Organize the paper in "triangular" or "newspaper" style, not in "joke" or "novel" style. Notice how newspapers start with the most important part, then ll in background later for the readers who kept going and want more details. A good joke or a mystery novel has a long windup to the nal punchline. Don't write papers like that-put the punchline right up front and then slowly explain the joke. Readers don't stick around to nd the punchline in Table 12. The vast majority of Ph.D. student papers and workshop presentations (not all by students !) get this exactly wrong, and we never really nd out what the contribution of the paper is until the last page, the last table, and the last 5 minutes of the seminar. A good paper is not a travelogue of your search process. We don't care how you came to gure out the right answer. We don't care about the hundreds of things you tried that did not work. Save it for your memoirs. Abstract Most journals allow 100-150 words. Obey this limit now. The main function of the abstract is to communicate the one central and novel contribution, which you just gured out. You should not mention other literature in the abstract. Like everything else, the abstract must be concrete. Say what you nd, not what you look for. Here too, don't write "data are analyzed, theorems are proved, discussion is made.."},
author = {Cochrane, John H},
file = {:C\:/Users/emeli/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cochrane - 2005 - Writing Tips for Ph. D. Students.pdf:pdf},
title = {{Writing Tips for Ph. D. Students}},
url = {http://gsbwww.uchicago.edu/fac/john.cochrane/research/Papers/},
year = {2005}
}
@article{Muller2021,
abstract = {Background: The increased availability and usage of modern medical imaging induced a strong need for automatic medical image segmentation. Still, current image segmentation platforms do not provide the required functionalities for plain setup of medical image segmentation pipelines. Already implemented pipelines are commonly standalone software, optimized on a specific public data set. Therefore, this paper introduces the open-source Python library MIScnn. Implementation: The aim of MIScnn is to provide an intuitive API allowing fast building of medical image segmentation pipelines including data I/O, preprocessing, data augmentation, patch-wise analysis, metrics, a library with state-of-the-art deep learning models and model utilization like training, prediction, as well as fully automatic evaluation (e.g. cross-validation). Similarly, high configurability and multiple open interfaces allow full pipeline customization. Results: Running a cross-validation with MIScnn on the Kidney Tumor Segmentation Challenge 2019 data set (multi-class semantic segmentation with 300 CT scans) resulted into a powerful predictor based on the standard 3D U-Net model. Conclusions: With this experiment, we could show that the MIScnn framework enables researchers to rapidly set up a complete medical image segmentation pipeline by using just a few lines of code. The source code for MIScnn is available in the Git repository: https://github.com/frankkramer-lab/MIScnn.},
archivePrefix = {arXiv},
arxivId = {1910.09308},
author = {M{\"{u}}ller, Dominik and Kramer, Frank},
doi = {10.1186/s12880-020-00543-7},
eprint = {1910.09308},
file = {::},
issn = {14712342},
journal = {BMC Medical Imaging},
keywords = {Biomedical image segmentation,Computer aided diagnosis,Deep learning,Medical image analysis,Open-source framework,U-Net},
month = {dec},
number = {1},
pmid = {33461500},
publisher = {BioMed Central Ltd},
title = {{MIScnn: a framework for medical image segmentation with convolutional neural networks and deep learning}},
volume = {21},
year = {2021}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {::},
month = {dec},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@article{Lu2017,
abstract = {The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-$2$) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-$(n+4)$ ReLU networks, where $n$ is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-$n$ ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth is more effective than width for the expressiveness of ReLU networks.},
archivePrefix = {arXiv},
arxivId = {1709.02540},
author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
eprint = {1709.02540},
file = {::},
month = {sep},
title = {{The Expressive Power of Neural Networks: A View from the Width}},
url = {http://arxiv.org/abs/1709.02540},
year = {2017}
}
@techreport{Zhang2021,
author = {Zhang, Aston and Lipton, Zachary C and Li, Mu and Smola, Alexander J},
file = {::},
title = {{Dive into Deep Learning Release 0.16.2}},
year = {2021}
}
@misc{Roy2017,
abstract = {Optical coherence tomography \(OCT\) is used for non-invasive diagnosis of diabetic macular edema assessing the retinal layers. In this paper, we propose a new fully convolutional deep architecture, termed ReLayNet, for end-to-end segmentation of retinal layers and fluid masses in eye OCT scans. ReLayNet uses a contracting path of convolutional blocks \(encoders\) to learn a hierarchy of contextual features, followed by an expansive path of convolutional blocks \(decoders\) for semantic segmentation. ReLayNet is trained to optimize a joint loss function comprising of weighted logistic regression and Dice overlap loss. The framework is validated on a publicly available benchmark dataset with comparisons against five state-of-the-art segmentation methods including two deep learning based approaches to substantiate its effectiveness.},
archivePrefix = {arXiv},
arxivId = {1704.02161},
author = {Roy, Abhijit Guha and Conjeti, Sailesh and Karri, Sri Phani Krishna and Sheet, Debdoot and Katouzian, Amin and Wachinger, Christian and Navab, Nassir},
booktitle = {arXiv},
doi = {10.1364/boe.8.003627},
eprint = {1704.02161},
file = {::},
issn = {23318422},
month = {apr},
pmid = {28856040},
publisher = {arXiv},
title = {{ReLaynet: Retinal layer and fluid segmentation of macular optical coherence tomography using fully convolutional networks}},
year = {2017}
}
@article{Maloca2019,
abstract = {Purpose To benchmark the human and machine performance of spectral-domain (SD) and sweptsource (SS) optical coherence tomography (OCT) image segmentation, i.e., pixel-wise classification, for the compartments vitreous, retina, choroid, sclera. Methods A convolutional neural network (CNN) was trained on OCT B-scan images annotated by a senior ground truth expert retina specialist to segment the posterior eye compartments. Independent benchmark data sets (30 SDOCT and 30 SSOCT) were manually segmented by three classes of graders with varying levels of ophthalmic proficiencies. Nine graders contributed to benchmark an additional 60 images in three consecutive runs. Inter-human and intra-human class agreement was measured and compared to the CNN results. Results The CNN training data consisted of a total of 6210 manually segmented images derived from 2070 B-scans (1046 SDOCT and 1024 SSOCT; 630 C-Scans). The CNN segmentation revealed a high agreement with all grader groups. For all compartments and groups, the mean Intersection over Union (IOU) score of CNN compartmentalization versus group graders' compartmentalization was higher than the mean score for intra-grader group comparison. Conclusion The proposed deep learning segmentation algorithm (CNN) for automated eye compartment segmentation in OCT B-scans (SDOCT and SSOCT) is on par with manual segmentations by human graders.},
author = {Maloca, Peter M. and Lee, Aaron Y. and {De Carvalho}, Emanuel R. and Okada, Mali and Fasler, Katrin and Leung, Irene and H{\"{o}}rmann, Beat and Kaiser, Pascal and Suter, Susanne and Hasler, Pascal W. and Zarranz-Ventura, Javier and Egan, Catherine and Heeren, Tjebo F.C. and Balaskas, Konstantinos and Tufail, Adnan and Scholl, Hendrik P.N.},
doi = {10.1371/journal.pone.0220063},
file = {::},
issn = {19326203},
journal = {PLoS ONE},
month = {aug},
number = {8},
pmid = {31419240},
publisher = {Public Library of Science},
title = {{Validation of automated artificial intelligence segmentation of optical coherence tomography images}},
volume = {14},
year = {2019}
}

@article{Alonso-Caneiro2013,
abstract = {The assessment of choroidal thickness from optical coherence tomography (OCT) images of the human choroid is an important clinical and research task, since it provides valuable information regarding the eye's normal anatomy and physiology, and changes associated with various eye diseases and the development of refractive error. Due to the time consuming and subjective nature of manual image analysis, there is a need for the development of reliable objective automated methods of image segmentation to derive choroidal thickness measures. However, the detection of the two boundaries which delineate the choroid is a complicated and challenging task, in particular the detection of the outer choroidal boundary, due to a number of issues including: (i) the vascular ocular tissue is non-uniform and rich in non-homogeneous features, and (ii) the boundary can have a low contrast. In this paper, an automatic segmentation technique based on graph-search theory is presented to segment the inner choroidal boundary (ICB) and the outer choroidal boundary (OCB) to obtain the choroid thickness profile from OCT images. Before the segmentation, the B-scan is pre-processed to enhance the two boundaries of interest and to minimize the artifacts produced by surrounding features. The algorithm to detect the ICB is based on a simple edge filter and a directional weighted map penalty, while the algorithm to detect the OCB is based on OCT image enhancement and a dual brightness probability gradient. The method was tested on a large data set of images from a pediatric (1083 B-scans) and an adult (90 B-scans) population, which were previously manually segmented by an experienced observer. The results demonstrate the proposed method provides robust detection of the boundaries of interest and is a useful tool to extract clinical data.},
author = {Alonso-Caneiro, David and Read, Scott A. and Collins, Michael J.},
doi = {10.1364/boe.4.002795},
file = {::},
issn = {2156-7085},
journal = {Biomedical Optics Express},
month = {dec},
number = {12},
pages = {2795},
pmid = {24409381},
publisher = {The Optical Society},
title = {{Automatic segmentation of choroidal thickness in optical coherence tomography}},
volume = {4},
year = {2013}
}

@inproceedings{Ronchetti2018,
abstract = {Macular Telangiectasia Type 2 (MacTel2) is a disease of the retina leading to a gradual deterioration of central vision. At the onset of the disease a good visual acuity is present, which declines as the disease progresses to cause reading difficulties. In this paper, we present new insights on the vascular changes in MacTel2. We investigated whether MacTel2 progression correlates to changes in the thickness of the choroid. For this purpose, we apply a recently published registration-based approach to detect deviations in the choroid on a dataset of 45 MacTel2 patients. Between 2012 and 2016 these subjects and a control group were measured twice within variable intervals of time in the Moorfields Eye Hospital in the MacTel Natural History Observation and Registry Study. Our results show that in the MacTel2 group the thickness of the choroid increased while in the control group a decrease was noted. Manual expert segmentation and an automated state-of-the-art method were used to validate the results.},
author = {Ronchetti, Tiziano and Maloca, Peter and de Carvalho, Emanuel Ramos and Heeren, Tjebo F.C. and Balaskas, Konstantinos and Tufail, Adnan and Egan, Catherine and Okada, Mali and Org{\"{u}}l, Selim and Jud, Christoph and Cattin, Philippe C.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-00949-6_36},
file = {::},
isbn = {9783030009489},
issn = {16113349},
keywords = {Choroidal thickness changes,Macular Telangiectasia Type 2,Piecewise rigid registration},
pages = {303--309},
publisher = {Springer Verlag},
title = {{Feasibility Study of Subfoveal Choroidal Thickness Changes in Spectral-Domain Optical Coherence Tomography Measurements of Macular Telangiectasia Type 2}},
volume = {11039 LNCS},
year = {2018}
}

@book{Ronchetti2017,
address = {Cham},
author = {Ronchetti, Tiziano and Maloca, Peter and Jud, Christoph and Meier, Christoph and Et al.},
doi = {10.1007/978-3-319-67561-9},
editor = {Cardoso, M. Jorge and Arbel, Tal and Melbourne, Andrew and Bogunovic, Hrvoje and Moeskops, Pim and Chen, Xinjian and Schwartz, Ernst and Garvin, Mona and Robinson, Emma and Trucco, Emanuele and Ebner, Michael and Xu, Yanwu and Makropoulos, Antonios and Desjardin, Adrien and Vercauteren, Tom},
file = {::},
isbn = {978-3-319-67560-2},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Fetal, Infant and Ophthalmic Medical Image Analysis}},
url = {http://link.springer.com/10.1007/978-3-319-67561-9},
volume = {10554},
year = {2017}
}
@techreport{Fabritius2001,
abstract = {This paper presents optical coherence tomography (OCT) signal intensity variation based segmentation algorithms for retinal layer identification. Its main ambition is to reduce the calculation time required by layer identification algorithms. Two algorithms, one for the identification of the internal limiting membrane (ILM) and the other for retinal pigment epithelium (RPE) identification are implemented to evaluate structural features of the retina. Using a 830 nm spectral domain OCT device, this paper demonstrates a segmentation method for the study of healthy and diseased eyes.},
author = {Fabritius, Tapio and Makita, Shuichi and Miura, Masahiro and Myllyl{\"{a}}, Risto and Yasuno, Yoshiaki and Koozekanani, D and Boyer, K and Roberts, C and Ishikawa, H and Stein, DM and Wollstein, G and Beaton, S and Fujimoto, JG and Schuman, JS and Szkulmowski, M and Wojtkowski, M and Sikorski, B and Bajraszewski, T and Srinivasan, V J and Szkulmowska, A and Kaluzny, J J and Fujimoto, J G and Kowalczyk, A},
booktitle = {Investigative Ophthalmol. Visual Scie},
file = {::},
pages = {2012--2017},
title = {{Image processing; (100.5010) Pattern recognition and feature ex-traction; (170.4470) Ophthalmology; (170.4500) Optical coherence tomography; (170.4580) Optical diagnostics for medicine}},
volume = {20},
year = {2001}
}
@article{Kugelman2019,
abstract = {The analysis of the choroid in the eye is crucial for our understanding of a range of ocular diseases and physiological processes. Optical coherence tomography (OCT) imaging provides the ability to capture highly detailed cross-sectional images of the choroid yet only a very limited number of commercial OCT instruments provide methods for automatic segmentation of choroidal tissue. Manual annotation of the choroidal boundaries is often performed but this is impractical due to the lengthy time taken to analyse large volumes of images. Therefore, there is a pressing need for reliable and accurate methods to automatically segment choroidal tissue boundaries in OCT images. In this work, a variety of patch-based and fully-convolutional deep learning methods are proposed to accurately determine the location of the choroidal boundaries of interest. The effect of network architecture, patch-size and contrast enhancement methods was tested to better understand the optimal architecture and approach to maximize performance. The results are compared with manual boundary segmentation used as a ground-truth, as well as with a standard image analysis technique. Results of total retinal layer segmentation are also presented for comparison purposes. The findings presented here demonstrate the benefit of deep learning methods for segmentation of the chorio-retinal boundary analysis in OCT images.},
author = {Kugelman, Jason and Alonso-Caneiro, David and Read, Scott A. and Hamwood, Jared and Vincent, Stephen J. and Chen, Fred K. and Collins, Michael J.},
doi = {10.1038/s41598-019-49816-4},
file = {::},
issn = {20452322},
journal = {Scientific Reports},
month = {dec},
number = {1},
pmid = {31527630},
publisher = {Nature Publishing Group},
title = {{Automatic choroidal segmentation in OCT images using supervised deep learning methods}},
volume = {9},
year = {2019}
}
@article{Zheng2020,
abstract = {Accurate segmentation of choroidal thickness (CT) and vasculature is important to better analyze and understand the choroid-related ocular diseases. In this paper, we proposed and implemented a novel and practical method based on the deep learning algorithms, residual U-Net, to segment and quantify the CT and vasculature automatically. With limited training data and validation data, the residual U-Net was capable of identifying the choroidal boundaries as precise as the manual segmentation compared with an experienced operator. Then, the trained deep learning algorithms was applied to 217 images and six choroidal relevant parameters were extracted, we found high intraclass correlation coefficients (ICC) of more than 0.964 between manual and automatic segmentation methods. The automatic method also achieved great reproducibility with ICC greater than 0.913, indicating good consistency of the automatic segmentation method. Our results suggested the deep learning algorithms can accurately and efficiently segment choroid boundaries, which will be helpful to quantify the CT and vasculature.},
author = {Zheng, Gu and Jiang, Yanfeng and Shi, Ce and Miao, Hanpei and Yu, Xiangle and Wang, Yiyi and Chen, Sisi and Lin, Zhiyang and Wang, Weicheng and Lu, Fan and Shen, Meixiao},
doi = {10.1142/S1793545821400022},
file = {::},
issn = {17937205},
journal = {Journal of Innovative Optical Health Sciences},
keywords = {Deep learning,choroid,segmentation,swept-source optical coherence tomography},
month = {jan},
publisher = {World Scientific},
title = {{Deep learning algorithms to segment and quantify the choroidal thickness and vasculature in swept-source optical coherence tomography images}},
year = {2020}
}
@article{He2021,
abstract = {Optical coherence tomography (OCT) is a noninvasive cross-sectional imaging technology used to examine the retinal structure and pathology of the eye. Evaluating the thickness of the choroid using OCT images is of great interests for clinicians and researchers to monitor the choroidal thickness in many ocular diseases for diagnosis and management. However, manual segmentation and thickness profiling of choroid are time-consuming which lead to low efficiency in analyzing a large quantity of OCT images for swift treatment of patients. In this paper, an automatic segmentation approach based on convolutional neural network (CNN) classifier and l2-lq (0<q<1) fitter is presented to identify boundaries of the choroid and to generate thickness profile of the choroid from retinal OCT images. The method of detecting inner choroidal surface is motivated by its biological characteristics after light reflection, while the outer chorioscleral interface segmentation is transferred into a classification and fitting problem. The proposed method is tested in a data set of clinically obtained retinal OCT images with ground-truth marked by clinicians. Our numerical results demonstrate the effectiveness of the proposed approach to achieve stable and clinically accurate autosegmentation of the choroid.},
author = {He, Fang and Chun, Rachel Ka Man and Qiu, Zicheng and Yu, Shijie and Shi, Yun and To, Chi Ho and Chen, Xiaojun},
doi = {10.1155/2021/8882801},
file = {::},
issn = {17486718},
journal = {Computational and Mathematical Methods in Medicine},
publisher = {Hindawi Limited},
title = {{Choroid Segmentation of Retinal OCT Images Based on CNN Classifier and l2 - Lq Fitter}},
volume = {2021},
year = {2021}
}
@article{Maloca2021,
abstract = {Machine learning has greatly facilitated the analysis of medical data, while the internal operations usually remain intransparent. To better comprehend these opaque procedures, a convolutional neural network for optical coherence tomography image segmentation was enhanced with a Traceable Relevance Explainability (T-REX) technique. The proposed application was based on three components: ground truth generation by multiple graders, calculation of Hamming distances among graders and the machine learning algorithm, as well as a smart data visualization (‘neural recording'). An overall average variability of 1.75% between the human graders and the algorithm was found, slightly minor to 2.02% among human graders. The ambiguity in ground truth had noteworthy impact on machine learning results, which could be visualized. The convolutional neural network balanced between graders and allowed for modifiable predictions dependent on the compartment. Using the proposed T-REX setup, machine learning processes could be rendered more transparent and understandable, possibly leading to optimized applications.},
author = {Maloca, Peter M. and M{\"{u}}ller, Philipp L. and Lee, Aaron Y. and Tufail, Adnan and Balaskas, Konstantinos and Niklaus, Stephanie and Kaiser, Pascal and Suter, Susanne and Zarranz-Ventura, Javier and Egan, Catherine and Scholl, Hendrik P.N. and Schnitzer, Tobias K. and Singer, Thomas and Hasler, Pascal W. and Denk, Nora},
doi = {10.1038/s42003-021-01697-y},
file = {::},
issn = {23993642},
journal = {Communications Biology},
month = {dec},
number = {1},
publisher = {Nature Research},
title = {{Unraveling the deep learning gearbox in optical coherence tomography image segmentation towards explainable artificial intelligence}},
volume = {4},
year = {2021}
}
@article{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
eprint = {1505.04597},
file = {:C\:/Users/emeli/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronneberger, Fischer, Brox - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:pdf},
month = {may},
title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
url = {http://arxiv.org/abs/1505.04597},
year = {2015}
}
@article{ronchetti2019,
abstract = {Monitoring subtle choroidal thickness changes in the human eye delivers insight into the pathogenesis of various ocular diseases such as myopia and helps planning their treatment. However, a thorough evaluation of detection-performance is challenging as a ground truth for comparison is not available. Alternatively, an artificial ground truth can be generated by averaging the manual expert segmentations. This makes the ground truth very sensitive to ambiguities due to different interpretations by the experts. In order to circumvent this limitation, we present a novel validation approach that operates independently from a ground truth and is uniquely based on the common agreement between algorithm and experts. Utilizing an appropriate index, we compare the joint agreement of several raters with the algorithm and validate it against manual expert segmentation. To illustrate this, we conduct an observational study and evaluate the results obtained using our previously published registration-based method. In addition, we present an adapted state-of-the-art evaluation method, where a paired t-test is carried out after leaving out the results of one expert at the time. Automated and manual detection were performed on a dataset of 90 OCT 3D-volume stack pairs of healthy subjects between 8 and 18 years of age from Asian urban regions with a high prevalence of myopia.},
author = {Ronchetti, Tiziano and Jud, Christoph and Maloca, Peter M. and Org{\"{u}}l, Selim and Giger, Alina T. and Meier, Christoph and Scholl, Hendrik P.N. and Chun, Rachel Ka Man and Liu, Quan and To, Chi Ho and Pova{\v{z}}ay, Boris and Cattin, Philippe C.},
doi = {10.1371/journal.pone.0218776},
file = {:C\:/Users/emeli/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ronchetti et al. - 2019 - Statistical framework for validation without ground truth of choroidal thickness changes detection.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
month = {feb},
number = {6},
pmid = {31251762},
publisher = {Public Library of Science},
title = {{Statistical framework for validation without ground truth of choroidal thickness changes detection}},
volume = {14},
year = {2019}
}

@online{WikiChoroid,
  title = {Choroïde},
  year = 2021,
  url = {https://fr.wikipedia.org/wiki/Choro%C3%AFde#Choro%C3%AFde_et_photographie},
}

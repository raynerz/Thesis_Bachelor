\documentclass[12pt,a4paper]{scrartcl}
\usepackage[english]{babel} %Für die indirekte Angabe von Umlauten. Es müssen dann Umlaute wie folgt im Code angegeben werden: "a "o "u "s.

\usepackage[utf8]{inputenc}
%Math/Physics
\usepackage{siunitx}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{braket}
\newcommand{\tens}[1]{% https://tex.stackexchange.com/questions/171788/always-have-the-ring-of-the-tensor-product-below-the-otimes -> Tensor Product
  \mathbin{\mathop{\otimes}\displaylimits_{#1}}%
}
%Page numbers
\usepackage{enumerate}
%Graphics
\usepackage{graphicx}
\usepackage{array}% http://ctan.org/pkg/array
\usepackage{floatrow}
\graphicspath{{./images/}}
\usepackage{lscape}
\usepackage{setspace}
\onehalfspacing
\usepackage{wrapfig}
\usepackage{hyperref}% für die Einbettung von Hyperlinks
\usepackage{subcaption}
\def\UrlBreaks{\do\/\do-}
\usepackage{multirow}
\usepackage{csquotes} %Quotations

\usepackage{scrwfile}

\usepackage{hyperref}


%Code
\usepackage{xcolor}
\definecolor{verde}{rgb}{0.25,0.5,0.35}
\definecolor{jpurple}{rgb}{0.5,0,0.35}
\usepackage{listings}
\lstset{
  language=Python,
  basicstyle=\footnotesize,
  keywordstyle=\color{jpurple}\bfseries,
  stringstyle=\color{red},
  commentstyle=\color{verde},
  morecomment=[s][\color{blue}]{/**}{*/},
  extendedchars=true,
  showspaces=false,
  showstringspaces=false,
  numbers=left,
  numberstyle=\tiny,
  breaklines=true,
  backgroundcolor=\color{cyan!10},
  breakautoindent=true,
  captionpos=b,
  xleftmargin=0pt,
  tabsize=4
}
% Appendices
\usepackage{pdfpages}


% Margins
%\usepackage{geometry} % Document Margins
%\setlength{\topmargin}{0cm}
%\setlength{\parindent}{5mm}
%\setlength{\parskip}{2mm}
%\setlength{\evensidemargin}{0mm}
%\setlength{\oddsidemargin}{0cm}
\pagestyle{headings}



\begin{document}
\thispagestyle{empty}
\vspace*{-3cm}
\begin{center}
\large \textsc{Bern University of Applied Sciences}
\vspace{0.5cm}
\hrule
\vspace{3.5cm}
{\Large \textsc{Written Report\\
Bachelor Thesis}}\\
{\large FS 2021}\\
\vspace{1cm}
{\Large \bfseries
Generation of Synthetic Ground Truth for Ophthalmic Medical Image Analysis}\\

\vspace*{1cm}

\end{center}


\begin{abstract}
As machine learning becomes ubiquitous in software engineering world, more and more projects are dedicated to implement this technology in the life sciences field. In this report, we present a framework that process ophthalmic OCT images and predicts synthetic vitreal, retinal, choroidal and scleral semantic segmentations. In this context, we combine image augmentation techniques and a convolutional neural network presenting an encoder-decoder structure. Validation was done by comparing our results with those obtained by manual expert segmentation. This evaluation showed, that our framework can automatically segment a regular OCT scan with a mean Jaccard coefficient  of 96.78\% when using image augmentation and 98.01\% without augmentation. In addition, we present a detailed workflow involving the utilisation of state-of-the-art machine learning techniques with comprehensive explanations on the data preparation, model implementation and evaluation criteria.  
\end{abstract}

\vspace{2cm}
\hspace*{5.2cm}
\parbox{8.2cm}

\begin{tabular}{ll}

Submitted by: & Emeline Lieberherr\\
& Rayner Zorrilla Alfonzo\\
Supervisor:  & Prof.~Dr.~Tiziano Ronchetti\\
Expert:  & Dr.~Harald Studer\\
Submission deadline: & Thursday, June 17th, 2021

\end{tabular}

\newpage
\pagenumbering{Roman}
\tableofcontents



\newpage
\pagenumbering{arabic}
%Und nun kommen wir zur Arbeit und fangen an die Seiten mit Arabischen Zahlen zu zählen

\section{Introduction}\label{s:introduction} 
\subsection{Motivation}\label{ss:motivation}
The choroid is the vascular layer of the eye. It contains connective tissues and is located between the sclera on the outside and the retina on the inside. The choroid is responsible for the irrigation and circulation of the ocular metabolism in order to supply the outer retina with oxygen and metabolites \cite{choroidExpl}.

Its thickness depends on several factors, including age, blood pressure, anatomic pathologies. While in adults it has been found that the thickness of the choroid decreases with age, the same has not yet been demonstrated in children and adolescents. In particular, subfoveal choroidal thickness has been found to be negatively correlated in Asian children, where the prevalence of myopia is higher \cite{Ronchetti2019}. Longitudinal studies of adolescents have shown that the eyeball lengthens during the development of myopia \cite{Ronchetti2018}. In the case of severe myopia this process is associated with a significant thinning of the choroidal thickness. Therefore the choroidal structure and thickness play a crucial role in monitoring the progression of myopia \cite{Ronchetti2019}.

The main challenge in detecting disease progression is to detect even small changes as early as possible. Optical coherence tomography (OCT) imaging allows the capture of highly resolved details of the retina and choroid to detect minute changes in the structure of both membranes\cite{Ronchetti2019}.

In order to measure changes in the choroidal thickness, it is necessary to detect all the different layers present in the ocular globe\textquotesingle s periphery and their interfaces with the help of OCT scans. With the help of the segmentation of follow-up images it is possible to compare results over several years. 
However, a manual approach raises two important problems: firstly, the number of scans to be processed (layer identification) is considerable \cite{Maloca2019}. Secondly, the low contrast, loss of signal, and the presence of other image degradations mentioned in Sec.~\ref{ss:motivation} the choroid makes it difficult to distinguish the sclera from the choroidal border \cite{Ronchetti2019}.

\subsection{Goal}
The aim of this project is to develop a deep learning model able to process ophthalmic OCT images and to predict  synthetic vitreal, retinal, choroidal and scleral semantic  segmentations based on a combination of image augmentation  techniques and a convolutional neural network presenting an encoder-decoder structure with a mean Jaccard coefficient of at least 95\%   \cite{Maloca2019}. This will automate the measurements made on the scans and considerably reduce the time needed to produce a result, thus reducing the workload of the experts and ensuring a high accuracy and standardization in the detection of the choroid. 

\subsection{Contributions}

The contribution of this project lies in the setup and documentation of a framework able to generate synthetic ground truth for ophthalmic image medical analysis where the vitreal, retinal, choroidal and scleral compartments are clearly defined. This framework includes image pre-processing, model implementation and results evaluation. 

From a medical perspective, the final output could offer ophthalmologists, as well as other healthcare professionals and researchers, a large dataset with accurately segmented retinal and choroidal layers. Such a dataset can be used as ground truth for various analyses focusing on choroidal and retinal thickness.

\subsection{Material and Methods}

\subsubsection{Methodology}

A Convolutional Neural Network (CNN) was designed based on the U-Net CNN model created by Prof.~Dr.~O.~Ronneberger in 2015 \cite{Ronneberger2015}. According to the general machine learning workflow, we used various techniques of data pre-processing that include vectorization, categorization and augmentation of the annotated images in order to train the network. Hyperparameters were modified both empirically from the result of several training sessions and by using best algorithmic practices for weighting and learning rate selection. Finally, the images resulting of the CNN predictions were evaluated with the support of different metrics commonly used in machine learning, such as the Jaccard coefficients, sensitivity and specificity. To evaluate the loss function we introduced a weighted version of the categorical cross entropy. 

\subsubsection{Tools}
The framework at its current state is dependent on the following tools : 
    
    \begin{table}[H]
    \begin{tabular}{ll}
    Tool & Version \\
    Python & 3.8.5 \\
    Keras     & 2.4.3   \\
    Tensorflow & 2.4.0 \\
    Opencv-python  &   4.5.1.48 \\
    Numpy & 1.19.2 \\
    Glob2  &  0.7 \\
    Matplotlib & 3.3.2
    \end{tabular}
    \end{table}

All trainings were performed in the Machine Learning Management Platform (MLMP) of the Berner Fachhochschule. The machine at disposal was a NVIDIA DGX Station A100 with 4 Tesla V100-DGXS-32GB GPUs. The operating system used was Ubuntu 18.04.5 LTS (Bionic Beaver).

\section{Medical Background}\label{s:medical_background}

The human eye is a sensory organ consisting of two different segments of spheres, the anterior and posterior segment \cite{snell1998}.

The anterior segment is the foremost part of the eye, it includes the cornea, iris and lens and serves to regulate the intensity of the light transmitted to the retina, which is located in the posterior segment of the eye next to the vitreous body, choroid and sclera. The function of the posterior segment is to convert light waves into electrical signals that are interpreted by the brain in the form of images \cite{Rhoades2017}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/anatomy-of-the-eye.jpg}
    \caption{Anatomy of the eye \cite{eyeanatomy-pic}}
    \label{fig:eye-anatomy}
\end{figure}

The area of interest of this project is limited to the posterior segment of the eyeball as the tissues therein are of significant clinical value to ophthalmologists in the analysis and diagnosis of refractive errors (myopia) and ocular diseases (glaucoma) \cite{Ronchetti2017}. In particular, we will focus on the vitreous body, retina, choroid and sclera as they will form part of the set of the manually annotated scans used in our project. 

The vitreous body is an structure characterized by a colorless and transparent gel. It consists mainly of water and other aminoacids, proteins, salts and acids. The vitreous is located between the lens and the retina and occupies 4/5 of the total volume of the eye. It serves mainly to transmit light from the lens to the retina and is also believed to contribute to the convergence power of the eye \cite{snell1998}. 

Once the light has traveled through the vitreous body, it reaches the retina, a layer of nervous tissue that covers the inside of the back two-thirds of the eyeball envolving the vitreous. As a part of the central nervous system, the retina converts light waves detected by its photoreceptors into electric signals that travel to the brain via the optic nerve \cite{purves2001}. In other terms, it converts light into electric signals that are sent to the brain and interpreted there as images.

Due to its vascular structure, the choroid\textquotesingle s irrigates the outer retina with oxigen through an intrinsic layer of capillaries   \cite{snell1998, choroidExpl}. Several studies \cite{Ronchetti2019, Ronchetti2018, Ho2013} indicate that choroidal thickness could be correlated with the development and progression of myopia and various ocular diseases. 

Finally, the sclera protects the inner part of the ocular globe through its collagenous structure. Together with the cornea, it serves to contain the internal pressure of the eye and also the forces created by the external muscles of the eye during eye movement \cite{Meek2008}.

\section{Technical Background}\label{s:TechBack}


\subsection{Optical Coherence Tomography}
Optical Coherence Tomography (OCT) is the state-of-the-art technique for producing accurate visualizations of ophthalmic medical images \cite{Garrido2014}. Since OCT uses near-infrared low-coherence light waves to produce measurements of the reflectivity as a function of depth (also known as A-scans\cite{Garrido2014}) it represents a non-invasive procedure to extract information from the ocular globe structure. Multiple A-scans from successive zones can be combined to produce a 2D image of the posterior segment of the eye, where the vitreous, retina, choroid and sclera are located. Finally, a contiguous set of B-scans produces a volumetric image that the ophtalmologist can use, firstly, to  evaluate the retinal and choroidal structure and, secondly, to calculate the layers thickness and make diagnoses for various types of diseases including several ocular diseases \cite{Ronchetti2019}, Diabetes \cite{Jiang2018}, Alzheimer\textquotesingle s disease, Glaucoma and other neurodegenerative diseases \cite{DENHAAN2017162}.   


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/csm_ABC_scans.png}
    \caption{Scan Types in Optical Coherence Tomography \cite{Willdeman2016}. From left to right: A-scan use a signal depth profile composed of time-gates reflections; B-scan use a frame composed of array of A-scans; Volume use a 3D dataset composed of array of B-scans.}
    \label{fig:mb-oct-abcscans}
\end{figure}

As stated in the motivation of this thesis, retinal and choroidal thickness can be measured from OCT scans using annotated ground truths establishing the locations of the tissues\textquotesingle boundaries. In the case of the retina, the Internal Limiting Membrane (ILM) divides it from the Vitreous at the upper boundary \cite{MACNAIR2015343}, while Bruch\textquotesingle s Membrane (BM) divides the retina from the choroid \cite{BOOIJ20101} serving as the retina\textquotesingle s bottom boundary. Subsequently, the Choroidal-Scleral Interface separates the choroid from the sclera \cite{Ronchetti2018}. The mentioned boundaries create a segmentation map within the OCT image, where the segmented vitreous body, retina, choroid and sclera look like as in Fig.~\ref{fig:annotated-oct-scan}. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/OCT-Scan.png}
    \caption{A B-scan with segmented layers from top to bottom:  Internal Limiting Membrane (ILM),  Bruch's Membrane (BM), Choroid-Sclera Interface (CSI). \cite{Ronchetti2019}}
    \label{fig:annotated-oct-scan}
\end{figure}

\subsection{Convolutional Neural Networks}\label{s:cnn}

A CNN is a powerful family of neural networks inspired by biological processes and containing Convolutional Layers, used in the field of deep learning. The design of the connection pattern is inspired by the structure of the neurons in the visual cortex of animals. Modern CNNs are effective for obtaining accurate models, as they require fewer parameters than fully connected architectures, and are often computationally more efficient because convolutions can be easily parallelized on GPU cores \cite{DIDLBook}. Furthermore, CNN-based architectures are widely used in the computer vision field, as they are prized for their efficiency. 

To be functional, Convolutional Networks require some operations; capturing features from images is achieved by applying a set of operations known as convolution and pooling, which are described in this chapter. 

If you want to process datasets by collecting and annotating pixels of images, you can quickly end up with a network that has huge dimensions, which leads to poor GPU performance. For this reason CNNs are used for data that is not tabular\footnote{By ``tabular'' we mean data consisting of rows corresponding to samples and columns corresponding to features.} \cite{DIDLBook}. The main role of the CNN is to facilitate the processing of images by reducing them to a form that is easier to examine without losing the features that are essential for good prediction.

\subsubsection{Convolution Operation}
An important concept of CNN is spatial invariance\footnote{Spatial invariance refers to the invariance of the model towards spatial transformations of images such as rotation, translation and scaling.}, which refers to the ability of a CNN model to recognize and identify features even when the input is transformed or slightly modified. In other words, object recognition is not sensitive with respect to their exact position in the image. In computer vision, adding convolution to the neural network allows the addition of an inductive visual prior whereby objects can appear anywhere. Sharing the weights over the location of the object significantly reduces the number of parameters to be learned, the convolution is shifted when the object changes its position, meaning that if the object on the image is moved, the convolution moves, making it equivariant to the translation  \cite{CNNSpatialLocation}. The earliest layers of our network should be implementing translation invariance, i.e., it should respond similarly regardless of where objects appears in the image. In addition, they should focus on local regions (``locality principle'') without regard to the content of the image in distant regions \cite{DIDLBook}.

The translation invariance implies that a shift in the input \(x\) should lead to a shift in the hidden representation \(H\) (i.e.~matrices in mathematics and two-dimensional tensors in code). 
This is a convolution, because to obtain the value \([H]_{i,j}\), we weight the pixels at \((i + a, j + b)\) near the location \((i, j)\) with coefficients \([V]_{a,b}\):
 
\begin{equation}
\label{eqn:invariance}
[H]_{i,j} = u + \sum_a\sum_b[V]_{a,b}[X]_{i+a,j+b}
\end{equation}
Note that X and H have the same shape while \([X]_{i,j}\) and \([H]_{i,j}\) denote the pixel at location \((i, j)\) in the input image and hidden representation. The translation invariance is only possible if V and u do not depend on \((i, j)\) \cite{DIDLBook}.
According to the aforementioned locality principle, it should not be necessary to look far away from the location \((i, j)\) to get information about what is happening in \([H]_{i,j}\). Outside a certain range \(|a| > \Delta \) or \(|b| > \Delta\), we should set  \([V]_{a,b} = 0\) and rewrite \([H]_{i,j}\) as follows \cite{DIDLBook}:
\begin{equation}
\label{eqn:locality}
[H]_{i,j} = u + \sum_{a=-\Delta}^{\Delta}\sum_{b=-\Delta}^{\Delta}[V]_{a,b}[X]_{i+a,j+b}
\end{equation}
The operations that provide spatial invariance (\ref{eqn:invariance}) and locality (\ref{eqn:locality}) are convolutions. In mathematics the convolution between two function \(f\) and \(g\) with \({ I\!R}^d \rightarrow { I\!R}\) is defined as follows \cite{DIDLBook}: 
\begin{equation}
(f*g)(x) = \int f(z)g(x-z)dz
\end{equation}
In other words, the convolution measures the overlap between \(f\) and \(g\) when one of the functions is “flipped” and shifted by \(x\). If we use two-dimensional tensors, we have a corresponding sum with indices \((a, b)\) for \(f\) and \((i-a, j-b)\) for \(g\), which is very similar to \ref{eqn:locality} (with the exception of using \((i+a, j+b)\) instead) \cite{DIDLBook}:
\begin{equation}
(f*g)(i,j) = \sum_{a}\sum_{b} f(a,b)g(i-a,j-b)
\end{equation}

\subsubsection{Pooling Operation}
In general, when processing images, it is more efficient to progressively reduce the detail of an image (for a given dimension) of our hidden representations. Thus, the higher one goes in the network, the larger the field of perception becomes, so the more the perceived image is composed of generalities rather than details. The final task we want to perform is, in the case of this work, a global one: to find out where the borderlines between the ILM, the BM and the CSI is. Therefore, the layers of our network should be sensitive to all the inputs in order to have a global view, this can be achieved by gradually accepting data and generating coarser maps but keeping all the advantages of convolutional layers at the intermediate processing layers. As with edge detection,  our representation must be invariant to translation, since changing just one pixel can change the image significantly. This is why pooling layers are used, which have the dual purpose of mitigating the location sensitivity of convolutional layers and spatially downsampling the representations. There are several types of (deterministic) pooling operations, here we discuss max-pooling and average-pooling \cite{DFTPooling}. Like convolutional layers, pooling operators operate on a fixed-size-window that moves through the different regions according to their step size \cite{DIDLBook}.
An example found in \cite{DIDLBook} shows how these operators work. If we take a 3$\times$3 matrix representing our input and imagine that the operator from the upper left corner from left to right and up and down, a max-pooling operation will create a new 2$\times$2 matrix: 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/pooling_example.png}
    \caption{Pooling Operation example  \cite{DIDLBook}}
\end{figure}
Basically the max-pooling operation consists of taking the maximum value of a part of matrix (left blue) and placing it in a new matrix (right) and then moving it and repeating this process until the new matrix is filled.
\[max(0, 1, 3, 4) = 4,\]
\[max(1, 2, 4, 5) = 5,\]
\[max(3, 4, 6, 7) = 7,\]
\[max(4, 5, 7, 8) = 8.\]


as well as average-pooling would have given:
\[average (0, 1, 3, 4) = 2,\]
\[average (1, 2, 4, 5) = 3,\]
\[average (3, 4, 6, 7) = 5,\]
\[average (4, 5, 7, 8) = 6.\]

\subsubsection{Softmax Operation}\label{tech:softmax}
The softmax function is a logistic function generalized to multiple dimensions and is used as an activation function for neural networks. It is used to normalize the input vector z of K real numbers to a distribution consisting of K probabilities over predicted output classes. In this project, we have four classes (vitreous, retina, choroid, and sclera), so the use of softmax is relevant because it is a classifier that has excellent performance for multi-classification tasks and is often used as the final layer of the network \cite{SoftMaxClassification}. 
The traditional softmax classifier can be described as follows: let K be the number of classes, \(x_i\) being the i$^\mathrm{th}$ imput feature with \(z_i\) being the label and \(z_j\) the j$^\mathrm{th}$ element of the vector of class scores, then we obtain \cite{SoftMaxClassification, DIDLBook}:

\begin{equation}
softmax(z_{ij}) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
\end{equation}
After repeating the convolution and pooling functions until we have a pixel to classify,  the softmax function is used to determine the probability that this pixel belongs to a particular class.

\subsection{U-net}\label{tech:unet}

The U-net is a CNN architecture developed for the field of biological images,  where there are few samples and the outputs consist of assigning specific regions of the image to one or more classes. Previously, a sliding window was the standard method to perform this task, but the method is computationally expensive and requires a huge amount of time depending on the image dimensions \cite{Ronneberger2015}. Therefore, in 2015 Prof.~Dr.~Olaf Ronneberger from the University of Freiburg in Germany developed an architecture that initially consists of a contracting or encoding path that uses a combination of convolutional and pooling operations in order to aggregate and classify pixels. Once the pixel is recognized, the architecture proposes a second expansive path using up-sampling operations, where the classified pixels are concatenated with features from the contracting or decoding path to add localization information. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/Unet-architecture.png}
    \caption{U-net architecture \cite{Ronneberger2015}. The blue boxes represent multichannel feature maps, while the arrows represent the different mathematical operations described in Sec.~\ref{s:cnn}.    }
\end{figure}

The architecture is faster and less expensive than sliding windows because pixel features are aggregated to avoid classifying them individually. The second major contribution of \cite{Ronneberger2015} is the idea of using image augmentation to overcome the problem of image sparseness. By augmenting the image dataset, we apply a number of common image transformation operations to slightly modify the base image and create synthetic copies that reinforce learning. The details of these operations are explained in Sec.~\ref{ss:datapreparation}.

\section{Previous Work}\label{s:prevWork}
\subsection{Computer Vision Methods for Detection of Early Retinal and Choroidal Thickness Changes in Children}

Computer vision methods for automated segmentation of inner retinal and choroidal layers obtained from OCT imaging \cite{Ronchetti2019statistic} can be used to generate a reliable set of annotated scans. 
The contribution of this project lies in the documentation and explanation of the most frequent methods for measuring retinal and choroidal thickness from an algorithmic perspective to provide a framework to easily extract features from OCT B-scans. 
There are several approaches for detecting retinal boundaries in OCT B-scans, among them we can cite pixel intensity variations, texture analysis and graph search-based segmentation techniques. Pixel intensity variations are often used when calculating the overall retinal thickness \cite{Alonso-Caneiro2013}, the method involves a series of computer graphic processing steps where each pixel of the image is compared to a threshold value chosen according to the color intensity of the pixels that contain the information about the location of the boundary \cite{Fabritius:09}. 
\\
In the case of the ILM, it can be easily extracted because the lens pixels in the OCT scans are mostly black, while the pixels of the membrane are mostly white, this is mainly due to the density of the element, the more dense the element is, the whiter it will appear in the image \cite{Brar597}. Therefore, taking the image from the top left corner, one can run a search algorithm that finds the first white pixel \footnote{The first pixel that is above the threshold.} from top to bottom and returns its position. Running this same algorithm across the $x$-axis produces an array of pixels that can be smoothed using cubic-spline interpolations, corresponding to the ILM. The process for extracting BM\textquotesingle s inner-most layer is the same: since the sclera has opaque pixels that can be omitted by the algorithm, the process would start in the bottom left corner and runs the algorithm from the bottom up looking for white pixels until it reaches the BM. The CSI is ignored in this works since the choroid\textquotesingle s boundary with the sclera is way too difficult to identify using pixel intensity variation. 
Retinal thickness was also calculated to output an overall average thickness and an average thickness in five zones, resulting the following image:

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{./images/ILM_BM_Extracted.png}
    \caption{Final rendering of the ILM, BM and retinal thickness}
    \label{fig:final_rendering}
\end{figure}

During this study, it has been confirmed that OCT scan quality can critically affect the result of an automated solution and potentially break the smoothing algorithm in the post-processing phase. When the scan had too much artifacts, it was almost impossible for the algorithm to correctly identify the ILM or BM. Using this method alone might result in a solution that cannot be fully automated, thus other methods, such as graph-based search must be explored. In this case, adjusting the natural smoothing spline parameter can improve the accuracy of the detected boundaries, but only in certain cases that depend entirely on the OCT scans. To automate the process of generating annotations for a machine learning model, either a manual approach or a semi-automated solution could provide better results that more accurately match the actual position of the membrane.  

\subsection{Literature Review}
There have been efforts to automate OCT scan segmentation using deep learning methods, although one of its principal limitations is described in Ronchetti et al.~\cite{Ronchetti2019} and Alonso-Caneiro et al.~\cite{Alonso-Caneiro2013} as a low degree of inter-observer agreement in detecting the CSI, which would lead to significant differences in class segmentation and consequently inconsistencies in the measurement of choroidal and retinal thickness. Furthermore, it was shown in \cite{Maloca2021} that the performance of deep learning methods is, at best, an average of the annotations of the experts.

In 2017, Roy et al.~\cite{Roy2017} proposed a fully convolutional deep learning architecture for segmenting  retinal layers and fluid masses in OCT scans called ReLayNet. The architecture is used to train a joint loss function consisting of weighted logistic regression and Dice overlap. A similar encoder-decoder architecture is proposed that uses convolutional and pooling operations and additionally adds a softmax operation block to classify pixels.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/relaynet-architecture.png}
    \caption{ReLaynet architecture \cite{Roy2017}.}
\end{figure}

The model was trained on a dataset consisting of 110 OCT B-scans of size 512$\times$740 pixels annotated by two experienced  ophtalmologists. Using a 8-fold cross validation, the results show a higher Dice overlap score compared to other commonly used methods, such as U-net or Fully Convolutional Networks (FCN).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/relaynet-predictions.png}
    \caption{OCT B-scan predictions using ReLaynet architecture model. On average, the proposed model had in case of the ILM a Dice overlap score of 0.90 compared to the 0.86 of U-net and 0,87 for the model known as Layer specific structured edge learning with a graph-search based dynamic programming  \cite{Roy2017}.}
\end{figure}

Maloca et al.~\cite{Maloca2019} validated the use of deep learning methods by implementing a U-net CNN trained on a dataset of 2070 B-scans. The data were manually annotated by segmenting 4 compartments corresponding to the vitreous, retina, choroid and sclera. This was done by identifying the ILM, the choriocapillaris and CSI. In \cite{Maloca2019}, the use of the choriocapillaris was preferred because BM is so thin in OCT scans that it cannot be detected.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/maloca-unet.png}
    \caption{Representation of the U-net model used in \cite{Maloca2019}.}
\end{figure}

The metric used to measure the performance of the U-net model in \cite{Maloca2019} was the Jaccard coefficient, also known as Intersection over Union (IoU), where all the matching pixels between both images are divided by the total of pixels of both images, giving a clear measure of similarity between both images. The results show mean Jaccard coefficient scores of 0.9929 for the vitreous, 0.9690 for the retina, 0.8819 for the choroid and 0.9768 for the sclera when benchmarked to a validation dataset of 60 images. From these results the authors concluded that the outputs of the proposed CNN were on par with manual segmentations made by human experts.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{./images/maloca-segmentations-results.png}
    \caption{Illustration of the predictions made by Maloca et al.~\cite{Maloca2019}. A spectral-domain OCT image (A) and a swept-source OCT image (C) were automatically segmented by the CNN (B,D) in to the compartments vitreous (arrow), retina (arrowheads), choroid (double arrowheads), and sclera (asterisk) \cite{Maloca2019}.}
\end{figure}


Zheng et al.~\cite{Zheng2020} proposed to use a modified U-net architecture for automated choroidal segmentation based on the Residual U-net model (ResNet) \cite{He2015}, which achieves higher performance with fewer parameters by using layers of residual functions. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{./images/ResNet-architecture.png}
    \caption{Residual U-net model proposed by \cite{He2015} and used in \cite{Zheng2020}.}
\end{figure}

The model was applied to a dataset composed by 450 OCT B-scans that were augmented to 1436 scans of 2048$\times$1561 pixels. The performance was measured using the intraclass correlation coefficient (ICC), which is described as the degree of similarity between two or more samples of membership in a common group defined by a particular set of features. In Zheng et al.~\cite{Zheng2020}, the compartments were conformed by choroidal boundaries BM, CSI and a set of vasculature measurements such as the choroidal vascularity index (CVI), choroidal stromal index (CStrI), luminal area (LA), stromal area (SA), the total choroidal area (TCA), and Choroidal Thickness (ChT). For the ChT the ICC between human and automated samples was 0.994 with a CoV of 2.284. For the parameters CVI, CStrI, LA, SA, and TCA, the value of ICC were 0.966, 0.977, 0.977, 0.964, 0.994 with CoValues of 2.230, 3.277, 1.653, 5.024, and 2.284, respectively.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{./images/choroidal-segmentation-zheng.png}
    \caption{Predictions obtained by Zheng et al.~\cite{Zheng2020} (a) OCT raw scan (b) automated identification of the upper and lower choroid boundaries, (c) binarization image of the choroid,  (d) delineation  of luminal area, (LA) and stromal area (SA) with red dotted line.}
\end{figure}





\section{Material and Method}\label{s:MaterialMethod}

In this section, we will detail the process of creating the framework from its inception point to evaluating the final output. In order to standardize its description we have chosen to fit our process to the machine learning workflow. 

\subsection{Data Collection}\label{ss:data_collection}
For this project we used a dataset composed of 755 OCT B-scans\footnote{See Sec.~\ref{s:TechBack} for more details.} of 500$\times$768 pixels. The data was obtained from Asian patients in the age range of 8-18 years stemming from urban regions with a high prevalence of myopia. The subjects were healthy with globally good distance and near vision, no systemic and ocular diseases, ocular trauma or surgery \cite{Ronchetti2019}. The images were acquired by a dual-wavelength eye-tracking OCT system operating simultaneously at the 870 and \SI{1075}{\nano\metre} bands. This system was developed by the HuCE-optoLab of the Bern University of Applied Sciences in Biel, Switzerland under the supervision of Prof.~Christoph Meier and Dr.~Boris Pova\v{z}ay\cite{Ronchetti2019}  before it was transferred and setup at the Hong Kong Polytechnic University\textquotesingle s School of Optometry\footnote{For more information about the OCT dataset we refer the interest reader to \cite{Ronchetti2019}}.

\subsubsection{Data Annotation}

The scans were annotated by a group of 6 ophthalmologists recruited as experts. Using an online tool they were able to draw the ILM, BM and CSI boundaries in the OCT scans (see \cite{Ronchetti2019} for more details).

\subsubsection{Mask Segmentation}
On the set of annotated scans, we ran an algorithm to merge the three separate layers into a single image, as depicted in Fig.~\ref{fig:merged}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{./images/ILM_BM_CSI_merged.png}
    \caption{Merged ILM, BM, CSI Layers.}
    \label{fig:merged}
\end{figure}

As a next step, we assigned a specific value to each pixel in the 4 respective compartments, thus obtaining a segmentation map, in which we can clearly delineate the vitreous, retina, choroid and sclera. Because of artifacts in data acquisition and a different number of annotated samples for each of the layers, not all scans could be processed by our algorithm, so the dataset was reduced to a total of 755 fully segmented B-scans.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{./images/chun_w00_fix_14_segmented_annotated.png}
    \caption{Segmentation Map used for training. \emph{Note that red elements are not part of the segmentation mask but has been added for clarity in this image}}
    \label{fig:annotated-layers}
\end{figure}

\subsection{Data Preparation}\label{ss:datapreparation}
First we define a size for the images we will feed to our model with, here we chose the size 256$\times$256 as this is the default size of images used in Keras \cite{chollet2015keras}. Also, we could not work with images that have a high resolution, as this would cause memory overflow during the training of the model. The data can be loaded via the method \emph{load\_data()} defined as follows:
\begin{lstlisting}[caption=The method for loading data can be found in the \emph{helper.py} file]
def load_data(datapath, is_train_data):
    images = []
    for directory_path in glob.glob(datapath):
        for img_path in sorted(glob.glob(os.path.join(directory_path, "*.tif"))):
            img = cv2.imread(img_path, 0)  # Read image as gray scale  
            zeros = np.zeros([config.IMAGES['IMG_SIZE_X']-config.IMAGES['IMG_SIZE_Y'], config.IMAGES['IMG_SIZE_X']]) 
            img = np.concatenate((zeros, img)) # Squaring the 500*768image to 768*768
            if(is_train_data):
                img = cv2.resize(img, (config.IMAGES['IMG_SIZE_TRAIN_Y'], config.IMAGES['IMG_SIZE_TRAIN_X']))
            else:
                img = cv2.resize(img, (config.IMAGES['IMG_SIZE_TRAIN_Y'], config.IMAGES['IMG_SIZE_TRAIN_X']), interpolation = cv2.INTER_NEAREST)  #Otherwise 
            images.append(img)
    #Convert list to array for machine learning processing        
    images = np.array(images)
    
    return images
\end{lstlisting}

We use \emph{LabelEncoder} \cite{scikit-learn} to encode our label with a value between 0 and n-classes-1, where n is the number of distinct labels, here we have four: the vitreous, retina, choroid and sclera. This allows us to add a weight to each of our classes and help to reduce class imbalance in our model. By ``class imbalance'' we mean that not all our classes have the same number of pixels; consequently, this can lead to bias in the model. If we look at the segmentation map in Fig.~\ref{fig:annotated-layers}, we can see that the vitreous covers much more pixels than the retina. Therefore, to achieve good performance, the model might tend to focus more on the vitreous pixels. So if one class has more pixels than another, it means that it has a higher weight in the model accuracy results than another because it has a higher percentage of pixels in the image. The goal is to balance the weighting of the classes to avoid this kind of situation which could lead to a biased model. To achieve this, we use the \emph{'balanced'} parameter of the \emph{sklearn} \cite{scikit-learn} \emph{compute\_class\_weight} method which automatically compute the weights using a logistic regression based algorithm \cite{scikit-learn} to balance our dataset. 

\begin{lstlisting}[caption={Encoding of the labels and weights attributions to the class, code from \emph{train.py} file}]
from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
n, h, w = train_masks.shape
train_masks_reshaped = train_masks.reshape(-1,1)
train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)
train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)

np.unique(train_masks_encoded_original_shape)

from sklearn.utils import class_weight

class_weights = class_weight.compute_class_weight(
                    'balanced',
                     np.unique(train_masks_reshaped_encoded),
                     train_masks_reshaped_encoded)
\end{lstlisting}

Normalization of a vector in mathematics means dividing the vector by its norm \cite{Normalization_Vector_Machines:2001}. In other words, the idea is to make the Euclidean length of the vector equal to one. In neural networks, normalization is important to obtain good results and reduce the calculation time \cite{Normalizazion_NN}, and is usually referd in the neural network literature as re-scaling the data in the same range of values (usually between 0 and 1) for each input feature to minimize bias \cite{Normalizazion_NN_classification}. Normalization is not a requirement for multi-layer perceptron but we still opted to do it anyway because our activation function, softmax (see sec.~\ref{tech:softmax}), has a range of [0,1], so we want to ensure that our target values lie within this range.

\begin{lstlisting}[caption={Normalize the data, the code can be found in the \emph{train.py} file}]
#Normalize Data
train_images = np.expand_dims(train_images, axis=3)
train_images = normalize(train_images, axis=1)

train_masks_input = np.expand_dims(train_masks, axis=3)
\end{lstlisting}

We can now split our data-set into two parts, one for the training and one for testing to estimate classification accuracy \cite{DatasetSplitting:2011}. We chose to split the dataset in a ratio of 80\% to 20\% \footnote{80\% of the images for the training set and the remaining 20\% for the validation set} in order to have images that the model does not process and carry out training validations.
\begin{lstlisting}[caption={Dataset split for testing and training from the \emph{train.py} file}, label={lst:data-split}]
# Picking 20% for testing and remaining for training
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(train_images, train_masks_input, test_size = 0.20, random_state = 0) 
\end{lstlisting}

Then we need to encode our label from categorical features to their numeric representation, as required for neural networks. We use \emph{to\_categorical} from Keras \cite{chollet2015keras} to convert our current class vector into its binary matrix representation.

\begin{lstlisting}[caption={Data conversion into categorical, code belongs in the \emph{train.py} file}, label={lst:data-categorical}]
from keras.utils import to_categorical
train_masks_cat = to_categorical(y_train, num_classes=n_classes)
y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))
\end{lstlisting}

Normally, deep learning requires a large amount of data to have good performances, but as we saw in Sec.~ \ref{ss:data_collection}, we only have 755 OCT B-scans. We already use U-net (Sec.~\ref{tech:unet}) to overcome this problem because it was specially designed to use small datasets. In addition, data augmentation can be added to artificially increase the number of data we feed to the model. Data augmentation is a widely adopted approach for increasing the amount of training data \cite{AugAndEval:2017} but it could come at the cost of a loss of quality, as too harsh modifications lead to loss of information in the data. This is even more true in our case because the OCT B-scans require micrometer precision (see Sec.~ \ref{ss:data_collection}), so we want to be careful in the choice of augmentation parameters.
We chose a rotation of no more than 5 degrees, because after some testing, we realized that more rotation would result in too much loss at the edges of the images, and since the scans are generally similar in the spatial positioning of the layers, this would mislead the model. To obtain a more consistent result, we used the \emph{"reflect" fill-mode}  to replace missing part of the images. In addition, we used an elastic function to compute the distortion. This function generates random displacement fields for height and width and smooths the fields with a Gaussian. 
\begin{lstlisting}[caption={Base generator for image augmentation, the definition of the 2 separate generators (image and mask) can be found in \emph{train.py}}]
train_datagen= ImageDataGenerator(rotation_range=5,
    fill_mode="reflect",
    shear_range=5,
    preprocessing_function=lambda x: elastic_transform(x, alpha_range=10, sigma=5)
)
\end{lstlisting}

We used data generator to feed our Keras model with augmented data in real time. Using data generator also means that if the dataset we use grows in the future, we avoid the problem of the dataset being loaded taking up a lot of memory.


\subsection{Model Development}

We based our model on the U-net architecture because of its flexibility to adapt to different types of images. We used a combination of an implementation made by Dr.~Ronchetti (as part of a side project by \cite{Ronchetti2019}) and the youtuber DigitalSreeni \cite{DigitalSreeni}.

According to the original U-net model, the contracting path is a combination of convolution and pooling operations as illustrated in Fig.~\ref{fig:model}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{./images/U-net_ThesisVersion.png}
    \caption{Model implementation based on U-net}
    \label{fig:model}
\end{figure}

Our first convolutional layer has 64 filters. As described in Lst.~\ref{lst:conv2D-1}, we used a 3$\times$3 kernel initialized by the keras initializer \emph{he\_normal}, which draws samples from a truncated normal distribution centered at 0 with $stddev = sqrt(2 / fan\_in)$, where \emph{fan\_in} is the number of input units in the weight tensor. The layer uses the same padding to avoid the loss of data information, and the activation function is the Rectified Linear Unit (relu), which fires the neuron for every positive value resulting from the convolution. 

It\textquotesingle s important to mention that to avoid overfitting, we dropped 10\% out of the output data that runs through the network and finally we applied a pooling operation with a 2$\times$2 kernel size in order to propagate the pixels with the most relevant features to the next neuronal layers.

\begin{lstlisting}[caption={Implementation of the first level of the contracting path, the rest of the layer can be found in the \emph{get\_unet()} method from \emph{model.py}}, label={lst:conv2D-1}]
inputs = Input((img_rows=256, img_cols=256,1))

c1 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)
c1 = Dropout(0.1)(c1)
c1 = Conv2D(filters=64, kernel_size= (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)
p1 = MaxPooling2D((2, 2))(c1)
\end{lstlisting}

The decoder part of the model is presented in Lst.~\ref{lst:conv2D-9} and involves the implementation of a transposed convolution which reconstructs the spatial dimensions of the image. The output is then concatenated with the layers of the contracting path, as described on Fig.~\ref{fig:model}, in order to add localization information\footnote{We refer the interested reader to Sec.~\ref{s:TechBack} for more detailed explanations.}.


Finally, the output is reduced to an image with 4 channels (one channel or class per each compartment in the OCT scan) and a softmax activation function determines the probability that the pixel belongs to each one of these classes and place the pixel in the correct channel.

\begin{lstlisting} [caption={Implementation of the last level of the upsampling path (full code in \emph{model.py})}, label={lst:conv2D-9}]
u9 = Conv2DTranspose(64, kernel_size=(2, 2), strides=(2, 2), padding='same')(c8)
u9 = concatenate([u9, c1], axis=3)
c9 = Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)
c9 = Dropout(0.1)(c9)
c9 = Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)
 
outputs = Conv2D(4, (1, 1), activation='softmax')(c9)
\end{lstlisting}
\subsection{Model Training}\label{ss:model_training}

The goal of training in deep learning algorithms is to minimize the loss or error of the gradient descent when optimizing the objective function. The learning rate specifies by how much the loss can be reduced to find a balance between the maximum accuracy of the learning process and the time invested \cite{Zhang2021}. It is a standard practice to find the learning rate by performing empiric experiments on the dataset. In our case, we used an algorithm to find the optimal learning rate according to our dataset. The implementation is provided by the Keras Learning Rate Finder \cite{chollet2015keras} and works by training the model against different learning rates and plotting the result of the loss against the learning rates. The rate where the slope of the curve generated is first negative and second more steep is the most efficient learning rate. 

\begin{lstlisting}[caption={Learning Rate Finder, the jupyter-notebook \emph{Learning\_Rate\_Finder.ipynb} is used to find the learning rate}, label={lst:learning-rate-finder}]
lr_finder = LRFinder(model)
lr_finder.find(X1, y_train_cat, start_lr=1e-17, end_lr=1, batch_size=32, epochs=10)
\end{lstlisting}

Given the model and the data selected, the algorithm finds that the most efficient learning rates are between 1e-4 and 1e-5, as shown in Fig.~ \ref{fig:lr_plot}. We therefore decided to use a learning rate of 1e-4 for our training, cross-validating the results with different learning rates and using the Keras callback ReduceLROnPlateau \cite{chollet2015keras}, which automatically reduces the learning rate, if it has not improved the loss during a given number of epochs.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{./images/learning_rate_plot.png}
    \caption{Learning rate plot using Keras LRFinder}
    \label{fig:lr_plot}
\end{figure}

\subsubsection{Model A with Data Augmentation}
In order to see what impact the data augmentation has on our dataset, we decided to train two different models. Model A implements data augmentation, as exposed in Sec.~\ref{ss:datapreparation} while Model B implements the raw dataset.

Model A fitting method can be seen in Lst.~\ref{lst:fit-method:A}. Since we use image augmentation and keras generator creates an unlimited number of images from our dataset, we decided to ``show'' the algorithm only 100 images per epoch. Thus, 400 epochs would mean that the model processes 40'000 images generated from a the training set of 604 images. Some authors \cite{Shorten2019, mikolajcyk2018} would argue that providing the model with such a large set of generated images would likely bias our results towards our own data distribution and the model would not be able to generalize further. However, since our dataset stems from a very limited subset of the Asian population, we believe that the model\textquotesingle s results would already be biased by design towards the anatomic characteristics  of Asian eyes. Nevertheless, we implemented a cross validation method using 20\% of the original dataset whose results indicate that our training is not overfitting the data, see Sec.~\ref{Results}. Therefore, we do not believe that our model\textquotesingle s ability to generalize the dataset is affected by the number of images generated, but the results are clearly biased towards the Asian population and would be probably more successful for that type of samples. The bias problem could only be solved by adding data from different populations so that the model learns the characteristics of the human eye from all possible different individuals. 

\begin{lstlisting}[caption={Model A training parameters, this is a code snippet from the  \emph{train\_model\_A} method in \emph{train.py}}, label={lst:fit-method:A}]
history = model.fit(train_generator,
                    steps_per_epoch = x_train.shape[0] // batch_size,
                    verbose=1, 
                    epochs=400, 
                    validation_data=(x_test, y_test_cat),
                    shuffle=True,
                    callbacks= [
                                ModelCheckpoint(monitor='val_loss',
                                filepath='weights/RMSProp_BCE_1e-4_best_weights.hdf5',
                                save_best_only=True,
                                save_weights_only=True),
                        ReduceLROnPlateau(monitor='val_loss',
                                          factor=0.1,
                                          patience=30,
                                          verbose=1,
                                          min_delta=1e-4),
                        TensorBoard(log_dir='./logs/syntethic_'+id ()),
                        CSVLogger('models/history'+id()+'.csv'),
                        EarlyStopping(monitor='val_loss',
                                      patience=70,
                                      verbose=1,
                                      min_delta=1e-4,
                                      baseline=0.2,
                                     )]
                   )
\end{lstlisting}

\subsubsection{Model B without Data Augmentation}

Model B was implemented over the raw dataset to cross validate the results and check the influence of image augmentation on our results. In this case, as seen in Lst. \ref{lst:fit-method-B} the input data comes directly from the numpy arrays created when splitting and converting the data into categorical data in Lst.~ref{lst:data-split} and \ref{lst:data-categorical}. 

\begin{lstlisting}[caption={Model B training parameters, this is a code snippet from the  \emph{train\_model\_B} method in \emph{train.py}},label={lst:fit-method-B}]
history = model.fit(x_train, y_train_cat,
                    batch_size=32,
                    verbose=1, 
                    epochs=100, 
                    validation_data=(X_test, y_test_cat), 
                    shuffle=True,
                    callbacks= [
                                ModelCheckpoint(monitor='val_loss',
                                filepath='weights/RMSProp_BCE_1e-4_best_weights.hdf5',
                                save_best_only=True,
                                save_weights_only=True),
                        ReduceLROnPlateau(monitor='loss',
                                          factor=0.1,
                                          patience=30,
                                          verbose=1,
                                          epsilon=1e-4),
                        TensorBoard(log_dir='./logs/syntethic_'+id),
                        CSVLogger('models/history'+id+'.csv'),
                        EarlyStopping(monitor='loss',
                                      patience=30,
                                      verbose=1,
                                      min_delta=1e-4,
                                      baseline=0.1,
                                      restore_best_weights=True)]
                   )

\end{lstlisting}

\subsection{Model Evaluation}
In order to evaluate the performance of our models we use several similarity measures, including the Jaccard coefficient, also called Intersection over Union (IoU), see Eq.~(\ref{eqn:Jaccard}). This measure is typically used to evaluate the similarity  between two arbitrary shapes and is invariant to the scale of the problem considered, because it encodes the shape-properties of the objects (like their width and height) and focuses the comparison of their areas or volumes.
\cite{IoU:2019}.

\begin{equation}
\label{eqn:Jaccard}
J(A,B) = \frac{|A\cap B|}{|A\cup B|}
\end{equation}

The Jaccard similarity coefficient between two datasets A and B is the result of dividing the number of common features between A and B by the total number of features of both datasets \cite{Niwattanakul2013}.
In image classification tasks that involve classifying individual pixels, this is equivalent to dividing  the number of pixels that overlap in the prediction and ground truth by the total number of pixels of the image. With this measure, we can compare the similarity between our model prediction and the ground truth by returning the percentage of correctly classified pixels. 
We have decided against accuracy as a performance measure in our project, since it does not provide enough detail for semantic segmentation tasks. Accuracy is measured by the number of correct predictions relative to the total number of predictions. Assuming we have 97\% accuracy, this means that 3\% of the pixels are not well classified. For a 500$\times$768 image this results in 11'250 misclassified pixels. This result provides no information about how the model performs at the class level. Furthermore, since in semantic segmentation it is common to have classes whose weight in the final image is higher than the others; such classes will be overrepresented in the final result compared to the minor classes. The reason is that  there are significantly more pixels that will be correctly classified than those within a class with low weight. 

Using a confusion matrix allows the performance of the model to be evaluated in a simple way by comparing the predicted values to the ground truth, and can be represented as follows \cite{ConfusionMatrix}:
    \begin{table}[H]
        \begin{tabular}{lll}
        \cline{2-3}
            \multicolumn{1}{l|}{}                                 & \multicolumn{1}{l|}{\textbf{predicted negative}} & \multicolumn{1}{l|}{\textbf{predicted positive}} \\ \hline
            \multicolumn{1}{|l|}{\textbf{ground truth negative}} & \multicolumn{1}{l|}{true negative (TN)}          & \multicolumn{1}{l|}{false positive (FP)}         \\ \hline
            \multicolumn{1}{|l|}{\textbf{ground truth positive}}  & \multicolumn{1}{l|}{false negative (FN)}         & \multicolumn{1}{l|}{true positive (TP)}          \\ \hline
        \end{tabular}
        \caption{Confusion matrix}
    \end{table}

With the confusion matrix we can calculate two metrics that we use for model evaluation: sensitivity and specificity. 
The sensitivity can be described as the proportion of correctly classified pixels among the true positive and false negatives, it shows the rate of relevant classified pixels classified. 

The specificity measures the ability to correctly reject the pixels that do not belong to a class. Consequently, specificity corresponds to the number of true negatives divided by the sum of the true negatives and false positives. Summarizing, while the sensitivity measures the proportion of true positive cases that were correctly predicted, the specificity can be used to determine the proportion of true negative cases that were correctly predicted, giving a sense of the ability of our model to reject pixels that do not belong to a particular class. 


 \section{Results and Discussion}

\begin{figure} \label{fig:results_modelA}
   \centering
   \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[trim= 100 250 80 0, clip, width=0.85\textwidth]{./images/results/A_syntethic_groundtruth_predictions29052021-112413_0.png}
    \caption{}
    \label{fig:results:A:a}
   \end{subfigure}
   \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[trim= 100 250 80 0, clip, width=0.85\textwidth]{./images/results/A_syntethic_groundtruth_predictions29052021-112417_4.png}
    \caption{}
    \label{fig:results:A:b}
   \end{subfigure}
    \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[trim= 100 250 80 0, clip, width=0.85\textwidth]{./images/results/A_syntethic_groundtruth_predictions29052021-112416.png}
    \caption{}
    \label{fig:results:A:c}
   \end{subfigure}
   \caption{Results of CNN prediction for the validation set of Model A}
  \label{Results}
\end{figure}

\begin{figure} 
   \centering
   \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[trim= 100 250 80 0, clip, width=0.85\textwidth]{./images/results/B_syntethic_groundtruth_predictions29052021-081035_0.png}
    \caption{}
    \label{fig:results:B:a}
   \end{subfigure}
   \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[trim= 100 250 80 0, clip, width=0.85\textwidth]{./images/results/B_syntethic_groundtruth_predictions29052021-081035_4.png}
    \caption{}
    \label{fig:results:B:b}
   \end{subfigure}
    \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[trim= 100 250 80 0, clip, width=0.85\textwidth]{./images/results/B_syntethic_groundtruth_predictions29052021-081035.png}
    \caption{}
    \label{fig:results:B:c}
   \end{subfigure}
   \caption{Results of CNN prediction for the validation set of Model B}
  \label{fig:results_modelB}
\end{figure}

After training and cross validation, Model A had a mean Jaccard coefficient of 96.78\% on the validation set. At a class level, the results were slightly better on the vitreous with 99.55\%, 96.97\% for retina, 94.16\% for choroid and 96.41\% for sclera.
The model had a mean of 97.80\% sensitivity referring to the overall ability to accurately classify relevant pixels. On the other hand, the mean specificity was 99.52\% indicating that the model had a high performance in rejecting pixels when they did not belong to a particular class. 

Model B presented a mean Jaccard coefficient of 98.01\%. At a class level the results were: 99.82\% for vitreous, 98.62\% for retina, 96.03\% for choroid and 97.59\% for sclera. Its average sensibility was 96.72\% while its specificity 99.65\%.

According to a similar research article \cite{Maloca2019}, the mean Jaccard coefficient of the CNN predictions obtained there was 99.29\% for the vitreous, 96.90\% for the retina, 88.17\% for the choroid and 97.68\% for the sclera, suggesting that the CNN compartmentalization of OCT scans was comparable to that of human graders \cite{Maloca2019}. 


\begin{figure}[H]
\centering
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./results/model_a_jaccard.png}
  \caption{Model A}
  \label{fig:model_a_jaccard}
\end{subfigure}
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./results/model_b_jaccard.png}
  \caption{Model B}
  \label{fig:modelb_jaccard}
\end{subfigure}
\caption{Jaccard Coefficient Results}
\label{fig:jaccard_results}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./results/model_a_sensitivity.png}
  \caption{Model A}
  \label{fig:model_a_sensitivity}
\end{subfigure}
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./results/model_b_sensitivity.png}
  \caption{Model B}
  \label{fig:modelb_sensitivity}
\end{subfigure}
\caption{Sensitivity Results}
\label{fig:sensitivity_results}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./results/model_a_specificity.png}
  \caption{Model A}
  \label{fig:model_a_specificity}
\end{subfigure}
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./results/model_b_specificity.png}
  \caption{Model B}
  \label{fig:modelb_specificity}
\end{subfigure}
\caption{Specificity Results}
\label{fig:specificity_results}
\end{figure}

In order to quantify the error between our models\textquotesingle  output and the ground truth, the loss function needs to be minimized, as described in Sec.~\ref{ss:model_training}. After 400 epochs, Model A had a training loss of 0.046 and the validation loss was 0.038. For Model B, the training lasted 100 epochs, the training loss was 0.016 while the validation loss was 0.027, as can be seen in Fig.~\ref{fig:modelloss}. The fact that the trend in these two metrics remained similar between the training and validation stage and also tended toward 0 indicates that our model neither overfits nor underfits the data \cite{DIDLBook}. 

\begin{figure}[H]
   \centering
   \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[ width=0.85\textwidth]{././images/results/modelA_loss_plot.png}
    \caption{Model A}
    \label{fig:loss_a}
   \end{subfigure}
   \begin{subfigure}{1\textwidth}
    \centering
    \includegraphics[ width=0.85\textwidth]{./images/results/modelB_loss_plot.png}
    \caption{Model B}
    \label{fig:loss_b}
   \end{subfigure}
   \caption{Loss function minimization}
   \label{fig:modelloss}
\end{figure}

With the aim of analyzing the robustness of our results, a series of statistical scores and tests were calculated. The pixel-precise information on the predictions allowed us to localize the tissue boundaries ILM, BM and CSI. Using this information, we calculated retinal and choroidal thickness for both the ground truth annotations and their predictions. The subsequent recording of the error, expressed in micrometers, explains how far the predictions were from the ground truth. The result of this comparison can be found in Table \ref{tab:Error_Scores}. The Mean Absolute Error (MAE) shows that both models have an average error in the range of the pixel scaling (\SI{3.87}{\micro\metre}) for both layers. The Root Mean Squared Error (RMSE) penalizes the extreme values, unlike the absolute version, so that the observed measurements were significantly larger when the data were averaged. This tell us that on average, when our models make an error, such an error can be quite significant, especially for Model A (trained using image augmentation) and the choroidal compartment of Model B. However, given the low mean MAE values, the boxplots and the scatterplots in  Fig.~\ref{fig:retinal_thickness_scatterplots} and \ref{fig:choroidal_thickness}, this was not often the case.

\begin{table}[H]
    \begin{tabular}{l c|c|c|c|c}
         \textbf{Model} &\textbf{Layer} & \textbf{Score}& \textbf{Error(\SI{}{\micro\metre})}  \\
         \hline
         A & Retina & Root Mean Squared Error &  8.5793 \\
         A & Choroid & Root Mean Squared Error & 25.4270 \\
         A & Retina & Mean Absolute Error & 2.8557 \\
         A & Choroid & Mean Absolute Error & 3.4331 \\
         B & Retina & Root Mean Squared Error & 2.2634 \\
         B & Choroid & Root Mean Squared Error & 14.5819 \\
         B & Retina & Mean Absolute Error & 1.0735 \\
         B & Choroid & Mean Absolute Error & 2.3225 \\
    \end{tabular}
    \caption{Root Mean Squared and Mean Absolute Errors}
    \label{tab:Error_Scores}
\end{table}


\begin{figure}[H]
\centering
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./results/A_boxplot_Absolute_Error_retina.png}
  \caption{Model A}
  \label{fig:boxplot_a_retinal_thickness}
\end{subfigure}
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./results/B_boxplot_Absolute_Error_retina.png}
  \caption{Model B}
  \label{fig:boxplot_b_retinal_thickness}
\end{subfigure}
\caption{Retinal thickness errors (in \SI{}{\micro\metre}), calculated as the absolute value of the difference between the manual expert annotations and the CNN predictions for the retinal boundaries ILM and BM. }
\label{fig:retinal_thickness_boxplots}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./results/A_boxplot_Absolute_Error_choroid.png}
  \caption{Model A}
  \label{fig:boxplot_a_choroidal_thickness}
\end{subfigure}
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./results/B_boxplot_Absolute_Error_choroid.png}
  \caption{Model B}
  \label{fig:boxplot_b_choroidal_thickness}
\end{subfigure}
\caption{Choroidal thickness errors (in \SI{}{\micro\metre}), calculated as the absolute value of the difference between the manual expert annotations and the CNN predictions for the choroidal boundaries BM and CSI.}
\label{fig:choroidal_thickness_boxplots}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./results/model_A_retinal_thickness.png}
  \caption{Model A}
  \label{fig:model_a_retinal_thickness}
\end{subfigure}
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./results/model_B_retinal_thickness.png}
  \caption{Model B}
  \label{fig:model_b_retinal_thickness}
\end{subfigure}
\caption{Retinal thickness regression analysis.}
\label{fig:retinal_thickness_scatterplots}
\end{figure}

\begin{figure}[H]
\centering
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./results/model_A_choroidal_thickness.png}
  \caption{Model A}
  \label{fig:model_a_choroidal_thicknesss}
\end{subfigure}
\begin{subfigure}{1\textwidth}
  \centering
  \includegraphics[width=\linewidth]{./results/model_B_choroidal_thickness.png}
  \caption{Model B}
  \label{fig:model_b_choroidal_thickness}
\end{subfigure}
\caption{Choroidal thickness regression analysis}
\label{fig:choroidal_thickness}
\end{figure}

\section{Conclusion}

When we analyze the results of Model A and Model B, we conclude that the influence of image augmentation on the training of CNNs is quite strong. Model B was trained during 100 epochs with a limited non-augmented dataset and was more capable of learning the features of the scans, showing a better Jaccard coefficient than Model A trained with augmentation (98.01\% vs.~96.78\% respectively). Image augmentation is used in this case to generate realistic transformations that can help the model to detect important features and to generalize the prediction to other type of OCT data even if in this case the evaluation measure takes a toll due to the randomness and the number of augmented images. Model B also has a higher degree of sampling bias because the amount of images is relatively small and the distribution of features might not be representative enough to detect certain kind of characteristics that are present in other OCT scans acquired either with other devices or from different ethnic groups.In general, sampling bias cannot be solved by artificially augmenting the images because the model can only learn from one type of population. In this case, machine learning engineers and healthcare professionals using such a machine learning system need to be aware of the data used to train the model and how it may affect the results with respect to the target population they are addressing.

The influence of inter- and intra- observer variability among experts in annotating data \cite{Maloca2019, Maloca2021, Ronchetti2019} can lead a bias in the outputs of the model \cite{Gabr2016}. The problem arises from the dependence of supervised machine learning on ground truth to learn the features in the training data. Our model is expected to rely heavily on the agreement of the selected experts on which features on OCT scans define the position of the different boundaries used to separate the layers. Unfortunately, this can have a great impact on the final results, as the model be as accurate as the ground truth provided.  

From a medical perspective, an automated system that segments the 4 compartments present in an OCT scan means a time savings in manually segmenting each B-scan. These time could be dedicated to enhance the care of the patient. However, since an operator bias is present in the outputs of the models, machine learning engineers must ensure that training is performed with data that meets high quality standards for segmentation accuracy.

Summarizing, our framework is able to learn the features needed to accurately segment the OCT scans. However, it was found that artifacts introduced due to different conditions during the scanning process (brightness, eye position, band selection) can have a negative impact on the learning process. Therefore, we could expect that an increase in the  quality of the scans would lead, firstly, to a lower inter- and intra- observer variability and, secondly, to a higher Jaccard coefficient, resulting in more accurate outputs.



\section{Future Work}

Further research on this topic would involve dedicating significant resources to create a more representative dataset so that the model can learn anatomical differences between different ethnic groups \cite{Consejo2020, Lin2009}. On the other hand, methods for accurately detecting the real position of every layer must be implemented to avoid operator bias and inter-observer variability. A suggested methodology might be to perform OCT scans on postmortem human eyes, manually annotate the segmentations from both the B-Scans and a posterior forensic analysis using an appropriate histopathology \cite{IOVINO2017, Mcnabb2009, Nioi2019} and correlate the data sources. Adding samples with this approach to our current dataset could improve the model learning process by avoiding the errors created by the presence of optical artifacts in the OCT scans, resulting in more accurate ground truth. It would be expected that by having a clearer image, the inter- and intra-observer variability would be minimal. This would help the model to identify features on boundaries that were missed in the current project as a result of the operator bias.

In order to improve the Jaccard coefficient of the models outputs, other machine learning models shall be implemented. Particularly, the implementation of residual functions (Resnet model) might be beneficial for the overall performance, as described in \cite{He2015, Zheng2020}. The implementation could also benefit from the application of statistical rigourosity with techniques that involve intra-grader or intra-model comparisons. 

Finally, stacking up a series of automatically segmented B-scans to obtain a C-scan would allow us to output volumetric measures that would directly allow ophtalmologists to come up with choroidal and retinal thickness without the manual annotation process of each B-scan generated by the OCT spectrometer. 

In conclusion, our results demonstrated that an automated generation of synthetic ground truth for ophthalmic medical image analysis is technically feasible: we developed a framework able to generate synthetic ground truth for ophthalmic medical image analysis.

\markboth{}{}

\newpage

\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{bibli} % Entries are in the "bibi.bib" file



\newpage

\appendix
\section{Appendices}
%\addcontentsline{toc}{section}{Appendices}
\renewcommand{\thesubsection}{\Alph{subsection}}
\subsection{Declaration of Independence}
\newpage
\markboth{}{}
  \normalsize
\begin{center}
\huge{\textbf{ Declaration of Independence}}\\[40mm]
\end{center}
\large
We confirm that the above work has been produced by the authors without any unauthorized assistance and without the use of any other means than those indicated, and that I have marked as such all passages that have been taken literally or meaningfully from published or unpublished writings.\\[30mm]
Bienne, the \today \\[10mm]
Emeline Lieberherr, Rayner Oswaldo Zorrilla Alfonzo
\begin{figure}[H]
    \includegraphics[width=0.5\textwidth]{./images/signatures.png}
\end{figure}
\newpage
\subsection{Project Architecture Diagram}

\includepdf[pages={1}]{./doc/appendix/projectArchitecture.pdf}
\newpage
\subsection{Project Planner}
\includepdf[pages={1}]{./doc/appendix/plannification.pdf}

\end{document}